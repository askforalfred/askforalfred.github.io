<!doctype html>
<html lang="en">
  <head>

    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-8024377-10', 'auto');
      ga('create', 'UA-25750945-3', 'auto', 'jesseTracker');
      ga('send', 'pageview');
      ga('jesseTracker.send', 'pageview');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Jekyll v3.8.5">
    <title>EAI @ CVPR 2021 </title>

    <link rel="canonical" href="http://askforalfred.com/EAI">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0">
    <meta name="twitter:title" content="ALFRED">
    <meta name="og:title" content="ALFRED">
    
    <meta name="description" content="A Benchmark for Interpreting Grounded Instructions for Everyday Tasks">
    <meta name="twitter:description" content="A Benchmark for Interpreting Grounded Instructions for Everyday Tasks">
    <meta property="og:description" content="A Benchmark for Interpreting Grounded Instructions for Everyday Tasks">

    <meta property="og:image" content="https://askforalfred.com/images/alfred_bg.jpg">
    <meta name="twitter:image" content="https://askforalfred.com/images/alfred_bg.jpg">
    <meta name="twitter:card" content="summary_large_image">
    
    <meta property="twitter:image:width" content="1200">
    <meta property="twitter:image:height" content="630">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">

    <script>
      /**
      * Function that captures a click on an outbound link in Analytics.
      * This function takes a valid URL string as an argument, and uses that URL string
      * as the event label. Setting the transport method to 'beacon' lets the hit be sent
      * using 'navigator.sendBeacon' in browser that support it.
       */
        var captureOutboundLink = function(url) {
          ga('send', 'event', 'outbound', 'click', url, {
            'transport': 'beacon'
          });
          ga('jesseTracker.send', 'event', 'outbound', 'click', url, {
            'transport': 'beacon'
          });
          // 'hitCallback': function(){document.location = url;}
          //});
          return false;
        }
    </script>

    <!--
    <script>
    // Set the date we're counting down to
    var countDownDate = new Date("Aug 5, 2021 0:0:0").getTime();
    
    // Update the count down every 1 second
    var x = setInterval(function() {
    
      // Get today's date and time
      var now = new Date().getTime();
    
      // Find the distance between now and the count down date
      var distance = countDownDate - now;
    
      // Time calculations for days, hours, minutes and seconds
      var days = Math.floor(distance / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
      var seconds = Math.floor((distance % (1000 * 60)) / 1000);
    
      // Display the result in the element with id="demo"
      document.getElementById("date").innerHTML = days + " days " + hours + ":"
      + minutes + ":" + seconds;
    
      // If the count down is finished, write some text
      if (distance < 0) {
        clearInterval(x);
        document.getElementById("date").innerHTML = "Aug 5, 2021";
      }
    }, 1000);
    </script>
    -->

    <style>
.bd-placeholder-img {
  font-size: 1.125rem;
  text-anchor: middle;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

    @media (min-width: 768px) {
      .bd-placeholder-img-lg {
        font-size: 3.5rem;
      }
    }

    #leaderboard 
    {
      width:500;
      height:400;
      position:absolute;
      overflow:hidden;
    }


    #innerIframe
    {
      position:relative;
      top:-550px;
      left:0px;
      width:500px;
      height:900px;
    }

    .performanceTable tr.human-row {
        background: #f5f5f5;
    }

    .performanceTable tr.random-row {
        background: #fafafa;
    }

    .performanceTable {
        margin-left: auto;
        margin-right: auto;
        max-width: 90%;
        border: 1px solid #ccc;
        background-color: #9da3bf14;
        border-radius: 4px;
    }

    .performanceTable .institution {
        font-style: italic;
        font-weight: 300;
    }

    .performanceTable .date {
        font-weight: normal;
        background-color: #777;
        display: inline;
        padding: .2em .6em .3em;
        font-size: 75%;
        line-height: 1;
        color: #fff;
        text-align: center;
        white-space: nowrap;
        vertical-align: baseline;
        border-radius: .25em;
    }

    .performanceTable td,
    .performanceTable th {
        text-align: center;
        max-width: 250px;
    }



    table tr *:nth-child(3) {
      border-right: 1px dashed #bfbfbf;
      border-left: 1px dashed #bfbfbf;
    }

    table tr *:nth-child(4) {
      border-right: 1px dashed #bfbfbf;
      border-left: 1px dashed #bfbfbf;
    }

    table tr *:nth-child(5) {
      border-right: 1px dashed #bfbfbf;
      border-left: 1px dashed #bfbfbf;
    }


/*    table tr *:nth-child(5) {
      border-right: 1px dashed #bfbfbf;
    }*/
    /*table tr td:nth-child(5) {*/
      /*border-right: 1px dashed #bfbfbf;*/
    /*}*/


    </style>

    <!-- Custom styles for this template -->
    <link href="https://getbootstrap.com/docs/4.3/examples/jumbotron/jumbotron.css" rel="stylesheet">
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../index.css">


  </head>


  <body style="padding-top: 0px;">
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
      <a class="navbar-brand" href="#">EAI @ CVPR 2021</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" 
                                                   data-target="#navbars" aria-controls="navbars" 
                                                                          aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbars">
        <div class="navbar-nav">
          <a class="nav-item nav-link active" href="#">Home <span class="sr-only">(current)</span></a>
          <a class="nav-item nav-link" href="https://askforalfred.com" 
              onclick="captureOutboundLink('https://askforalfred.com');">ALFRED <img src="../images/alfred-white.svg" width=50></a>
          <a class="nav-item nav-link" href="https://github.com/askforalfred/alfred/tree/master/data" target="_blank" 
              onclick="captureOutboundLink('https://github.com/askforalfred/alfred/tree/master/data');">Data</a>
          <a class="nav-item nav-link" href="https://github.com/askforalfred/alfred" target="_blank" 
              onclick="captureOutboundLink('https://github.com/askforalfred/alfred');">Code</a>
        </div>
      </div>
    </nav>

    <main role="main">

    <!-- Main jumbotron -->

    <div class="jumbotron">
      <div class="row">
        <div class="col-md-1"></div>
        <div class="col-md-5">
          <h1>ALFRED Challenge</h1><br><br>
            <div class="row">
            <div class="col-md-3" align="center">
              <img width=100 src="photos/ALFRED.png">
            </div> <!-- col 7-->
            <div class="col-md-6" align="center">
              <img width=400 src="photos/cvpr21_logo.jpg">
            </div> <!-- col 5-->
          </div> <!-- row -->
          <br><br>
          <a href="https://leaderboard.allenai.org/alfred/submissions/public">Leaderboard</a> is accepting submissions to the ALFRED <a href="https://embodied-ai.org/" target="_blank" onclick="captureOutboundLink('https://embodied-ai.org/');">Embodied AI Challenge</a> with <code>[EAI21]</code> tags!  Humans have a success rate of 91% on unseen environments, but the best models are still far behind üò¢ <br/>
          <br/>
          Can you do even better? <a href="https://github.com/askforalfred/alfred">Code, precomputed features, and AI2Thor simulator are all available for a quick start on GitHub</a>
          <br/>
          <br/>
          <b>The submission deadline has been extended to Jun 7!</b>
        </div> <!--col-->
        <div class="col-md-5">
          <!-- <h1>Deadline in <b id="date">August 5</b></h1><br> -->
	  <h1>Leaderboard</h1>
          <div id="leaderboard">
            <a class="btn btn-primary btn-lg homeButton" id="leaderboardButtonEval" href="https://leaderboard.allenai.org/alfred/submissions/public" role="button" target="_blank" onclick="captureOutboundLink('https://leaderboard.allenai.org/alfred/submissions/public');">Leaderboard¬ª</a>
            <iframe src="https://leaderboard.allenai.org/alfred/submissions/public" id="innerIframe" scrolling="yes"></iframe>
          </div>
        </div> <!--col-->
      </div> <!-- row -->
    </div>
    </main>
      <br>

<div class="container">
      <h2><a href="https://workshopsandtutorials.eccv2021.eu/papers/category/workshop-sunday-aug-23/embodied-vision-actions-language/" target="_blank" onclick="captureOutboundLink('https://workshopsandtutorials.eccv2021.eu/papers/category/workshop-sunday-aug-23/embodied-vision-actions-language/');">ALFRED Challenge @ CVPR 2021 Embodied AI Workshop</a></h2>
      <br>
      <div class="row">
            <iframe width="100%" height="600" id="ytVid" src="https://www.youtube.com/embed/1XoRLNmXffo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div> <!-- /container -->
            
    <hr>

    <div class="container">
      <br>
        <br>
        <div class="container">
          <h3><b>Overview</b></h3>
          <div class="row">
            <div class="col-md-6">
              <br>
              <p> 
              The focus of this challenge is on embodied visual tasks that require the grounding of language to actions in real-world settings. Specifically, we want to draw focus to challenges like partial observability, continuous state spaces, and irrevocable actions for language-guided agents in visual environments.
              Such challenges are not captured by current datasets for embodiment [<a href="https://bringmeaspoon.org/">1</a>, <a href="https://homes.cs.washington.edu/~xkcd/papers/iqa.html">2</a>, <a href="https://embodiedqa.org/">3</a>].
              </p>
            </div> <!-- col 7-->
            <div class="col-md-6">
              <ul><b>Key Topics</b>
                <li>Egocentric and Robotic vision</li>
                <li>Language Grounding</li>
                <li>Navigation and Path Planning</li>
                <li>Interactive/Causal Reasoning</li>
                <li>Learning from Demonstration</li>
                <li>Task and Symbolic Planning</li>
                <li>Deep Reinforcement Learning</li>
                <li>Commonsense Reasoning</li>
              </ul>
            </div> <!-- col 5-->
          </div> <!-- row -->
          <div class="row">
            <div class="col-md-10">
              To encourage research in embodied vision & language, the workshop includes a benchmark challenge based on <a href="http://askforalfred.com">ALFRED</a>.
              This benchmark captures real-world complexities like object state changes, and requires long-horizon planning. This workshop exists to bring together Vision, Robotics, and NLP researchers to tackle the unique challenges of this three-field intersection that are often avoided when focusing only on vision-and-language or vision-and-robotics (i.e., 'embodied AI').
            </div> <!-- /col -->
              <div class="col-md-10">
              <br>
              The previous version of this challenge was held at <a href="https://askforalfred.com/EVAL/">ECCV 2020</a> where the top scoring submission attained an Unseen Success Rate of 4.5%.
            </div> <!-- /col -->
          </div> <!-- /row -->
        </div> <!-- container -->
      <br>
      <div class="container">
          <div class="row">
            <div class="col-md-10">
                <table style="display: inline-block;"><tr><td>
                <a href="https://mohitshridhar.com/" target="_blank" 
                   onclick="captureOutboundLink('https://mohitshridhar.com/');">
                   <center><img src="photos/mohitshridhar.png" height=100><br>Mohit Shridhar</a></center>
                </td></tr></table>
                <table style="display: inline-block;"><tr><td>
                <a href="https://jessethomason.com/" target="_blank" 
                   onclick="captureOutboundLink('https://jessethomason.com/');">
                   <center><img src="photos/jessethomason.jpg" height=100><br>Jesse Thomason</a></center>
                </td></tr></table>
                <table style="display: inline-block;"><tr><td>
                <a href="https://yonatanbisk.com/" target="_blank" 
                   onclick="captureOutboundLink('https://yonatanbisk.com/');">
                   <center><img src="photos/YonatanBisk.jpg" height=100><br>Yonatan Bisk</a></center>
                </td></tr></table>
                <table style="display: inline-block;"><tr><td>
                <a href="http://roozbehm.info/" target="_blank" 
                   onclick="captureOutboundLink('http://roozbehm.info/');">
                   <center><img src="photos/roozbehmottaghi.jpg" height=100><br>Roozbeh Mottaghi </a></center>
                </td></tr></table>
                <table style="display: inline-block;"><tr><td>
                <a href="https://prior.allenai.org/people" target="_blank" 
                   onclick="captureOutboundLink('https://prior.allenai.org/people');">
                   <center><img src="photos/EricKolve.jpg" height=100><br>Eric Kolve</a></center>
                </td></tr></table>
              </div>
            </div>
        </div> <!-- container -->

      <hr>

    <div class="container">
      <div class="row">
        <div class="col-md-6">
          <h4>Challenge Details</h4>
          <p><b>Guidelines</b> <br>Participants are required to upload their model to  our  <a href="https://leaderboard.allenai.org/alfred/submissions/public" target="_blank" onclick="captureOutboundLink('https://leaderboard.allenai.org/alfred/submissions/public');">evaluation  server</a>  with <code>[EAI21]</code> in the submission title, e.g. <i>[EAI21] Seq2seq Model</i>.   The  evaluation  server  automatically  evaluates  the models on an unseen test set.  Final numbers for the prize challenge will be frozen on <del>May 31</del> Jun 7. Winning submissions will be required to submit a <b>brief (private) report of technical details</b> for validity checking. We will also <b>conduct a quick code inspection</b> to ensure that the challenge rules weren't violated (e.g. using additional info on test scenes).</p>
          <p><b>Dataset</b><br> The challenge is based on the <a href="https://github.com/askforalfred/alfred" target="_blank">ALFRED Dataset</a>, which contains 25K language annotations of both high-level goals and low-level step-by-step instructions for various tasks set in the <a href="https://ai2thor.allenai.org/" target="_blank">AI2THOR</a> simulator. Agents interact with environments through discrete actions and pixelwise masks. </p>
        </div> <!-- col 5 -->

        <div class="col-md-4">
          <h4>Important Dates</h4>
          <table>
            <tr><td colspan=2><hr align=left width=250><b>Timeline</b></td></tr>
            <tr><td><del>Leaderboard closes</del> </td><td> <del>May 31</del></td></tr>
            <tr><td>Leaderboard closes </td><td> Jun 7</td></tr>
            <tr><td>Winner announcement  </td><td> Jun 19</td></tr>
          </table>
        </div> <!-- col 5 --> 
      </div> <!-- row -->
    </div> <!-- container -->
    <div class="container">
      <div class="row">
        <div class="col-md-11">
          <p><b>Evaluation</b><br> The leaderboard script records actions taken by a pre-trained agent and dumps them to a JSON file. These deterministic actions in the JSON will be replayed on the leaderboard server for evaluation. This process is model-agnostic, allowing you to use your local resources for test-time inference. </p>
          <p><b>Metric</b><br> The submissions will be ranked by Unseen Success Rate. </p>
          <p><b>Rules</b><br> 
          <ol>
            <li>Include <code>[EAI21]</code> in the submission title e.g. <i>[EAI21] Seq2seq Model</i>.</li>
            <li>Do not exploit the metadata in test-scenes: you should solve the vision-language grounding problem without misusing the metadata from THOR. For leaderboard evaluations, agents should just use <b>RGB input</b> and <b>language instructions (goal & step-by-step)</b>. You <b>cannot use additional depth, mask, metadata info etc. from the simulator on Test Seen and Test Unseen scenes.</b> Submissions that use additional info on test scenes will be disqualified. 
            However, during training you are allowed to use additional info for auxiliary losses etc.</li>
            <li>During evaluation, agents are restricted to <code>max_steps=1000</code> and <code>max_fails=10</code>. Do not change these settings in the <a href="https://github.com/askforalfred/alfred/blob/master/models/eval/leaderboard.py#L220" target="_blank">leaderboard script</a>; these modifications will not be reflected in the evaluation server.</li>
            <li>You can publish your results on the leaderboard only once every 7 days.</li>
            <li>Try to solve the ALFRED dataset: all submissions must be attempts to solve the ALFRED dataset.</li>
            <li>Answer the following questions: a. Did you use additional sensory information from THOR as input, eg: depth, segmentation masks, class masks, panoramic images etc. during test-time? If so, please report them. b. Did you use the alignments between step-by-step instructions and expert action-sequences for training or testing? (no by default; the instructions are serialized into a single sentence)</li>
            <li>Share who you are: you must provide a team name and affiliation.</li>
            <li>(Optional) Share how you solved it: if possible, share information about how the task was solved. Link an academic paper or code repository if public.</li>
            <li>Only submit your own work: you may evaluate any model on the validation set, but must only submit your own work for evaluation against the test set.</li>
          </ol>
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-md-6">
          <p>
            <b>Prizes</b> <br>
          </p>
          <h5>
            üèÜ <b>1st Place ‚Äì 1,500 USD</b> Amazon Gift Card <br><br> ü•à <b>2nd Place ‚Äì 500 USD</b> Amazon Gift Card 
          </h5>
        </div> <!-- col 5 -->

        <div class="col-md-4">
          Sponsored by:
          <img src="photos/amazon.svg" height=100>
        </div> <!-- col 5 --> 
      </div> <!-- row -->
    </div> <!-- container -->
    <div class="container">

    <hr>

    <div class="container">
        <div class="container">
          <h3><b>Submissions</b></h3>
          <p>Pending code inspection.</p>
          <br>
          <div class="col-md-13">
               <table class="table performanceTable box-shadow">
                    <tbody>
                    <tr>
                        <th>Rank</th>
                        <th style="min-width: 25%;">Model</th>
                        <th style="max-width: 10%;">Unseen Success</th>
                        <th style="max-width: 10%;">Seen Success</th>
                        <th style="max-width: 10%;">Unseen PLW</th>
                        <th style="max-width: 10%;">Seen PLW</th>
                    </tr>
                    <tr class="human-row">
                        <td></td>
                        <td>Human Performance<p class="institution">University of Washington</p><a
                                href="https://arxiv.org/abs/1912.01734" target="_blank" >(Shridhar et al. '20)</a></td>
                        <td>91.0</td>
                        <td>-</td>
                        <td>87.6</td>
                        <td>-</td>
                    </tr>
<tr><td><p>ü•á</p><span class="date">07 Jun, 2021</span></td><td>HLSM<p class="institution">Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi</p></td><td>16.3</td><td>25.1</td><td>4.3</td><td>6.7</td></tr>
<tr><td><p>2</p><span class="date">06 Jun, 2021</span></td><td>ABP<p class="institution">Byeonghwi Kim, Suvaansh Bhambri, Kunal Pratap Singh, Roozbeh Mottaghi, Jonghyun Choi. <br>(GIST, IIT Roorkee, AI2)</p></td><td>15.4</td><td>44.5</td><td>1.1</td><td>3.9</td></tr>
<tr><td><p>3</p><span class="date">30 Jan, 2021</span></td><td>HiTUT<p class="institution">Yichi Zhang and Joyce Chai (University of Michigan)</p></td><td>13.9</td><td>21.3</td><td>5.9</td><td>11.1</td></tr>
<tr><td><p>4</p><span class="date">04 Jan, 2021</span></td><td>LWIT<p class="institution">Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani</p></td><td>9.4</td><td>30.9</td><td>5.6</td><td>25.9</td></tr>
<tr><td><p>5</p><span class="date">07 Jun, 2021</span></td><td>SRCB-sudoer<p class="institution"> Xingrui Wang. HaoyuLiu and YangLiu,<br> Samsung Research, China</p></td><td>5.6</td><td>21.3</td><td>2.7</td><td>14.8</td></tr>
<tr><td><p>6</p><span class="date">06 Jun, 2021</span></td><td>EmBERT<p class="institution"> Anonymous </p></td><td>5.0</td><td>31.5</td><td>2.2</td><td>24.3</td></tr>
<tr><td><p>-</p><span class="date">28 Mar, 2020</span></td><td>Seq2seq Baseline<p class="institution">University of Washington</p><a class="link" target="_blank"  href="https://github.com/askforalfred/alfred">github.com/askforalfred/alfred</a></td><td>0.4</td><td>4.0</td><td>0.0</td><td>2.0</td></tr>
                    </tbody>
                </table>
          </div> <!-- col 7-->
        </div>
        <br>
    </div>
    <hr>

    <div class="container">
      <div class="row">
        <div class="col-md-11">
          <h4>FAQ</h4>
            <ul>
              <li><b>Do you need to submit a report?</b> <br>Winning submissions will be required to submit a brief (private) report of technical details for validity checking. Also consider submitting a workshop paper to EAI by May 13. See <a href="https://embodied-ai.org/" target="_blank">submission guidelines for EAI</a>.</li><br>
              <li><b>Is there a prize for the winner?</b><br>Yes! 1,500 USD Amazon Gift Card for First Place, and 500 USD Amazon Gift Card for Second Place.</li><br>
            </ul>
          </h4>
        </div>
      </div>
    </div>
    <hr>

    <div class="container">
      <div class="row">
        <div class="col-md-11">
          <h4>Sponsorship Provided by</h4>
            <img src="photos/amazon.svg" height=100>
          </h4>
        </div>
      </div>
    </div>

    <footer class="container">
      <p>Questions or issues?  Contact <a href="mailto:askforalfred@googlegroups.com">askforalfred@googlegroups.com</a></p>
    </footer>
  </body>
</html>

